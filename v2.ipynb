{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5228 Mini Project: Customer Segmentation and Churn Prediction\n",
    "\n",
    "**Objective:** This project aims to analyze a telecom customer churn dataset to identify customer segments and predict churn using data mining techniques.\n",
    "\n",
    "**Goals:**\n",
    "1.  **Unsupervised Learning:** Segment customers based on usage patterns using K-Means and DBSCAN.\n",
    "2.  **Supervised Learning:** Predict customer churn using various classification models (Logistic Regression, Decision Tree, Random Forest, KNN, XGBoost) and optimize their performance.\n",
    "3.  **Insights:** Identify key drivers of churn and provide actionable recommendations for customer retention.\n",
    "\n",
    "**Dataset:** Telecom Customer Churn dataset (provided via BigML split into 80% train, 20% test). Contains 20 columns (19 features + 1 target 'Churn').\n",
    "\n",
    "**Methodology:**\n",
    "*   Data Loading & Initial Inspection\n",
    "*   Data Preprocessing (Encoding, Scaling, Feature Engineering/Selection)\n",
    "*   Exploratory Data Analysis (EDA)\n",
    "*   Unsupervised Clustering (K-Means, DBSCAN)\n",
    "*   Supervised Classification (Model Training, Hyperparameter Tuning, Evaluation)\n",
    "*   Model Interpretation & Actionable Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.13.1' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Step 0: Setup and Data Loading ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, make_scorer, classification_report\n",
    "\n",
    "# --- Configuration ---\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# --- Load Data ---\n",
    "train_file_path = 'churn-bigml-80.csv'\n",
    "test_file_path = 'churn-bigml-20.csv'\n",
    "\n",
    "print(f\"Loading training data from: {train_file_path}\")\n",
    "try:\n",
    "    train_df_raw = pd.read_csv(train_file_path)\n",
    "    print(\"Training data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Training file not found at {train_file_path}.\")\n",
    "    train_df_raw = pd.DataFrame() # Avoid errors later\n",
    "\n",
    "print(f\"\\nLoading testing data from: {test_file_path}\")\n",
    "try:\n",
    "    test_df_raw = pd.read_csv(test_file_path)\n",
    "    print(\"Testing data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Testing file not found at {test_file_path}.\")\n",
    "    test_df_raw = pd.DataFrame() # Avoid errors later\n",
    "\n",
    "# --- Initial Inspection ---\n",
    "if not train_df_raw.empty:\n",
    "    print(\"\\n--- Training Data ---\")\n",
    "    print(\"\\n1. First 5 rows:\")\n",
    "    print(train_df_raw.head())\n",
    "    print(\"\\n2. Dataset Info:\")\n",
    "    train_df_raw.info()\n",
    "    print(\"\\n3. Missing Values:\")\n",
    "    print(train_df_raw.isnull().sum())\n",
    "    print(\"\\n4. Shape:\")\n",
    "    print(train_df_raw.shape)\n",
    "\n",
    "if not test_df_raw.empty:\n",
    "    print(\"\\n\\n--- Testing Data ---\")\n",
    "    print(\"\\n1. First 5 rows:\")\n",
    "    print(test_df_raw.head())\n",
    "    print(\"\\n2. Dataset Info:\")\n",
    "    test_df_raw.info()\n",
    "    print(\"\\n3. Missing Values:\")\n",
    "    print(test_df_raw.isnull().sum())\n",
    "    print(\"\\n4. Shape:\")\n",
    "    print(test_df_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Setup and Data Loading - Results\n",
    "\n",
    "*   **Data Loaded:** Both `churn-bigml-80.csv` (2666 rows) and `churn-bigml-20.csv` (667 rows) were loaded successfully into `train_df_raw` and `test_df_raw` respectively. Each has 20 columns.\n",
    "*   **Data Types:** A mix of numerical (`int64`, `float64`) and categorical (`object`) features were identified. Categorical columns needing encoding are 'State', 'International plan', 'Voice mail plan', and the target 'Churn'. 'Area code' is numerical but will be treated as categorical.\n",
    "*   **Missing Values:** No missing values were found in either the training or testing dataset.\n",
    "*   **Target Variable:** The 'Churn' column (currently object type: 'True'/'False') is the target for prediction.\n",
    "\n",
    "**Next Step:** Proceed with Data Preprocessing to prepare the data for analysis and modeling. This involves encoding categorical variables and scaling numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing\n",
    "\n",
    "**Objective:** Transform the raw data into a suitable format for machine learning algorithms. This involves converting categorical data to numerical representations and scaling numerical features to prevent certain features from dominating due to their range.\n",
    "\n",
    "**Tasks:**\n",
    "1.  **Copy Data:** Create copies of the raw dataframes to avoid modifying the originals.\n",
    "2.  **Binary Encoding:** Convert 'International plan', 'Voice mail plan', and 'Churn' columns from Yes/No or True/False into 1/0 integers.\n",
    "3.  **One-Hot Encoding:** Convert nominal categorical features ('State', 'Area code') into numerical format using one-hot encoding. This creates new binary columns for each category, avoiding ordinal assumptions. `drop_first=True` will be used to prevent multicollinearity among dummy variables.\n",
    "4.  **Align Columns:** Ensure the training and testing datasets have the exact same columns after one-hot encoding, adding missing columns with 0s if necessary.\n",
    "5.  **Feature Scaling:** Scale numerical features using `StandardScaler` to have zero mean and unit variance. The scaler will be fit *only* on the training data and then used to transform both training and test sets to prevent data leakage.\n",
    "6.  **Separate Features/Target:** Isolate the target variable ('Churn') from the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Data Preprocessing ---\n",
    "\n",
    "# Copy dataframes\n",
    "train_df = train_df_raw.copy()\n",
    "test_df = test_df_raw.copy()\n",
    "\n",
    "# 1. Encode Binary Categorical Features\n",
    "binary_cols_map = {'Yes': 1, 'No': 0, True: 1, False: 0}\n",
    "for col in ['International plan', 'Voice mail plan', 'Churn']:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].map(binary_cols_map)\n",
    "    if col in test_df.columns:\n",
    "        test_df[col] = test_df[col].map(binary_cols_map)\n",
    "# Convert Churn explicitly to int\n",
    "if 'Churn' in train_df.columns: train_df['Churn'] = train_df['Churn'].astype(int)\n",
    "if 'Churn' in test_df.columns: test_df['Churn'] = test_df['Churn'].astype(int)\n",
    "print(\"Binary columns encoded.\")\n",
    "\n",
    "# Separate Target Variable BEFORE one-hot encoding and scaling\n",
    "y_train = train_df['Churn']\n",
    "y_test = test_df['Churn']\n",
    "X_train_interim = train_df.drop('Churn', axis=1)\n",
    "X_test_interim = test_df.drop('Churn', axis=1)\n",
    "\n",
    "# Identify column types for encoding and scaling\n",
    "categorical_cols = ['State', 'Area code']\n",
    "# Identify numerical columns (excluding binary ones already handled and Area Code)\n",
    "numerical_cols = X_train_interim.select_dtypes(include=np.number).columns.tolist()\n",
    "numerical_cols.remove('Area code') # Remove area code as it's treated categorical\n",
    "# Remove binary cols we already mapped (they don't need scaling)\n",
    "binary_numeric_cols = ['International plan', 'Voice mail plan']\n",
    "numerical_cols = [col for col in numerical_cols if col not in binary_numeric_cols]\n",
    "\n",
    "print(f\"Categorical columns for OHE: {categorical_cols}\")\n",
    "print(f\"Numerical columns for Scaling: {numerical_cols}\")\n",
    "print(f\"Binary columns (0/1): {binary_numeric_cols}\")\n",
    "\n",
    "\n",
    "# 2. Create Preprocessing Pipelines (Handles OHE, Scaling, and Alignment implicitly)\n",
    "# This is a more robust way than applying get_dummies/StandardScaler separately\n",
    "\n",
    "# Preprocessor for numerical data: Scaling\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessor for categorical data: One-Hot Encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # handle_unknown='ignore' helps with unseen categories in test set\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (like the binary ones) untouched\n",
    ")\n",
    "\n",
    "# 3. Apply the preprocessor\n",
    "print(\"\\nApplying preprocessing pipeline (Fit on Train, Transform Train & Test)...\")\n",
    "# Fit on training data and transform training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train_interim)\n",
    "\n",
    "# Transform test data using the SAME fitted preprocessor\n",
    "X_test_processed = preprocessor.transform(X_test_interim)\n",
    "\n",
    "# Get feature names after transformation (important for later steps)\n",
    "# Handle numerical, categorical (one-hot encoded), and remainder columns\n",
    "num_features = numerical_cols\n",
    "cat_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
    "# Identify remainder columns (those passed through)\n",
    "remainder_cols = [col for col in X_train_interim.columns if col not in numerical_cols and col not in categorical_cols]\n",
    "processed_feature_names = list(num_features) + list(cat_features) + list(remainder_cols)\n",
    "\n",
    "\n",
    "# Convert processed arrays back to DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_processed, columns=processed_feature_names, index=X_train_interim.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_processed, columns=processed_feature_names, index=X_test_interim.index)\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessing complete.\")\n",
    "print(\"Shape of processed training features:\", X_train_scaled.shape)\n",
    "print(\"Shape of processed testing features:\", X_test_scaled.shape)\n",
    "print(\"\\nFirst 5 rows of processed training data:\")\n",
    "print(X_train_scaled.head())\n",
    "print(\"\\nData types of processed training data:\")\n",
    "X_train_scaled.info()\n",
    "print(\"\\nVerification: Mean and Std Dev of scaled numerical columns:\")\n",
    "print(X_train_scaled[numerical_cols].agg(['mean', 'std']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing - Results & Summary\n",
    "\n",
    "*   **Encoding Complete:** Binary features ('International plan', 'Voice mail plan', 'Churn') were mapped to 1/0. Nominal features ('State', 'Area code') were successfully One-Hot Encoded.\n",
    "*   **Scaling Complete:** Numerical usage features were scaled using `StandardScaler`. The mean of these features in the processed training data is ~0 and the standard deviation is ~1, confirming successful scaling.\n",
    "*   **Robust Preprocessing:** Using `ColumnTransformer` and `Pipeline` ensures consistent application of scaling and encoding, automatically handling feature alignment between train and test sets.\n",
    "*   **Final Feature Set:** The processing resulted in 68 features (original numerical + OHE categories + binary features). The target variable `y_train`/`y_test` is separate.\n",
    "*   **Data Ready:** The data (`X_train_scaled`, `X_test_scaled`, `y_train`, `y_test`) is now in a purely numerical format suitable for EDA and machine learning models.\n",
    "\n",
    "**Next Step:** Perform Exploratory Data Analysis (EDA) on the *original* and *processed* training data to understand feature distributions, correlations, and identify potential redundancies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Objective:** Gain insights into the data, understand relationships between features, and identify patterns related to customer churn *before* building complex models.\n",
    "\n",
    "**Tasks:**\n",
    "1.  **Target Variable Analysis:** Visualize the distribution of 'Churn' to understand class balance.\n",
    "2.  **Correlation Analysis (Features vs. Target):** Calculate and visualize the correlation between each feature (using scaled data) and the 'Churn' variable to identify potential predictors.\n",
    "3.  **Feature Distributions:** Visualize the distributions of key numerical features (using original, unscaled data for interpretability), potentially grouped by churn status, using histograms or box plots.\n",
    "4.  **Feature Correlation Matrix (Multicollinearity):** Calculate and visualize the correlation matrix between all processed features (`X_train_scaled`) to identify highly correlated features (potential redundancy).\n",
    "5.  **Identify Significant Features (Preliminary):** Based on correlation with the target, make a preliminary identification of the most influential features for churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Exploratory Data Analysis (EDA) ---\n",
    "print(\"--- EDA on Training Data ---\")\n",
    "\n",
    "# 1. Target Variable Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y_train)\n",
    "plt.title('Distribution of Churn Variable (0: No Churn, 1: Churn)')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Count')\n",
    "churn_rate = y_train.mean()\n",
    "plt.text(0.5, max(y_train.value_counts())*0.9, f'Churn Rate: {churn_rate:.2%}', ha='center')\n",
    "plt.show()\n",
    "print(f\"Churn Rate: {churn_rate:.2%}\")\n",
    "\n",
    "# 2. Correlation with Target Variable\n",
    "print(\"\\nCalculating correlations with the target variable (Churn)...\")\n",
    "# Combine scaled features and target for correlation calculation\n",
    "X_train_scaled_with_target = X_train_scaled.copy()\n",
    "X_train_scaled_with_target['Churn'] = y_train\n",
    "correlations_with_target = X_train_scaled_with_target.corr()['Churn'].sort_values(ascending=False).drop('Churn') # Drop self-correlation\n",
    "\n",
    "print(\"\\nTop 10 Features MOST Positively Correlated with Churn:\")\n",
    "print(correlations_with_target.head(10))\n",
    "print(\"\\nTop 10 Features MOST Negatively Correlated with Churn:\")\n",
    "print(correlations_with_target.tail(10))\n",
    "\n",
    "# Visualize top correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_corr_features_plot = pd.concat([correlations_with_target.head(10), correlations_with_target.tail(10)])\n",
    "sns.barplot(x=top_corr_features_plot.values, y=top_corr_features_plot.index, palette='vlag')\n",
    "plt.title('Top Features Correlated with Churn')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Visualize Distributions of Key Numerical Features by Churn Status\n",
    "print(\"\\nVisualizing distributions of key original numerical features by Churn status...\")\n",
    "key_numerical_features_orig = [\n",
    "    'Total day minutes', 'Customer service calls',\n",
    "    'Total eve minutes', 'Total intl calls', 'Number vmail messages'\n",
    "]\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(key_numerical_features_orig):\n",
    "    plt.subplot(1, len(key_numerical_features_orig), i + 1)\n",
    "    sns.boxplot(data=train_df_raw, x='Churn', y=col) # Use raw data for original scale\n",
    "    plt.title(f'{col} by Churn')\n",
    "    plt.xlabel('Churn (False/True)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. Feature Correlation Matrix (Multicollinearity Check)\n",
    "print(\"\\nCalculating feature correlation matrix (heatmap)...\")\n",
    "correlation_matrix = X_train_scaled.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10)) # Adjusted size slightly\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, annot=False, fmt=\".1f\", linewidths=.5, xticklabels=False, yticklabels=False) # Hide labels for overview\n",
    "plt.title('Feature Correlation Matrix Heatmap (Scaled Training Data)')\n",
    "plt.show()\n",
    "\n",
    "# Identify pairs with high correlation\n",
    "print(\"\\nIdentifying highly correlated feature pairs (|correlation| > 0.9)...\") # Using 0.9 threshold\n",
    "highly_correlated_pairs = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname_i = correlation_matrix.columns[i]\n",
    "            colname_j = correlation_matrix.columns[j]\n",
    "            pair = tuple(sorted((colname_i, colname_j)))\n",
    "            highly_correlated_pairs.add(pair)\n",
    "\n",
    "if highly_correlated_pairs:\n",
    "    print(\"Highly correlated feature pairs found:\")\n",
    "    for pair in sorted(list(highly_correlated_pairs)):\n",
    "        print(f\"- {pair[0]} and {pair[1]}: {correlation_matrix.loc[pair[0], pair[1]]:.3f}\")\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found (|correlation| > 0.9).\")\n",
    "\n",
    "# Specifically check Voice mail plan vs Number vmail messages correlation\n",
    "# Note: Voice mail plan is binary (0/1) in the 'remainder' columns now\n",
    "if 'Voice mail plan' in X_train_scaled.columns and 'Number vmail messages' in X_train_scaled.columns:\n",
    "     vmail_corr = correlation_matrix.loc['Voice mail plan', 'Number vmail messages']\n",
    "     print(f\"\\nCorrelation between 'Voice mail plan' and 'Number vmail messages': {vmail_corr:.3f}\")\n",
    "     if abs(vmail_corr) > 0.9:\n",
    "         print(\"=> High correlation detected. Consider removing one.\")\n",
    "         # Decision: Keep 'Number vmail messages' as it's more granular\n",
    "         col_to_drop_redundant = 'Voice mail plan'\n",
    "     else:\n",
    "         col_to_drop_redundant = None\n",
    "else:\n",
    "    print(\"\\nCould not calculate voice mail correlation (columns missing).\")\n",
    "    col_to_drop_redundant = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Exploratory Data Analysis (EDA) - Results & Summary\n",
    "\n",
    "*   **Class Imbalance:** The target variable 'Churn' is imbalanced, with a churn rate of approximately 14.5%. This means accuracy alone is not a sufficient evaluation metric; F1-score, precision, and recall are more informative.\n",
    "*   **Key Correlates with Churn:**\n",
    "    *   *Positive Correlation (Higher churn risk):* `International plan`, `Customer service calls`, `Total day minutes`.\n",
    "    *   *Negative Correlation (Lower churn risk):* `Voice mail plan`, `Number vmail messages`, `Total intl calls`.\n",
    "*   **Feature Distributions:** Box plots confirmed that customers who churned tend to have higher `Total day minutes` and significantly more `Customer service calls`, while having fewer `Number vmail messages` and slightly fewer `Total intl calls`.\n",
    "*   **Multicollinearity:**\n",
    "    *   The heatmap showed strong correlations as expected (e.g., between usage minutes and their corresponding charges, though charges were removed earlier).\n",
    "    *   A very high correlation (0.957) was confirmed between `Voice mail plan` and `Number vmail messages`.\n",
    "\n",
    "**Decision:** Based on the high correlation, we will remove the `Voice mail plan` feature to reduce redundancy, keeping the more granular `Number vmail messages`.\n",
    "\n",
    "**Next Step:** Apply the redundancy removal and then proceed to Unsupervised Learning (Customer Segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1b: Data Preprocessing - Redundancy Removal\n",
    "\n",
    "**Objective:** Remove features identified during EDA as highly correlated to prevent multicollinearity issues, which can affect some models' performance and interpretation.\n",
    "\n",
    "**Task:** Drop the `Voice mail plan` column from the processed training and test feature sets, based on its high correlation (0.957) with `Number vmail messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1b: Redundancy Removal ---\n",
    "\n",
    "col_to_drop_redundant = 'Voice mail plan' # Confirmed from EDA\n",
    "\n",
    "if col_to_drop_redundant and col_to_drop_redundant in X_train_scaled.columns:\n",
    "    print(f\"\\nDropping redundant feature: '{col_to_drop_redundant}'\")\n",
    "    X_train_final = X_train_scaled.drop(columns=[col_to_drop_redundant])\n",
    "    X_test_final = X_test_scaled.drop(columns=[col_to_drop_redundant])\n",
    "    print(f\"Shape after dropping '{col_to_drop_redundant}':\")\n",
    "    print(\"Train:\", X_train_final.shape)\n",
    "    print(\"Test:\", X_test_final.shape)\n",
    "else:\n",
    "    print(f\"\\nColumn '{col_to_drop_redundant}' not found or no column to drop. Using previous data.\")\n",
    "    X_train_final = X_train_scaled.copy()\n",
    "    X_test_final = X_test_scaled.copy()\n",
    "\n",
    "# Define data for clustering (use the final processed training features)\n",
    "X_train_cluster = X_train_final.copy()\n",
    "\n",
    "# Ensure y_train and y_test are still the correct targets\n",
    "y_train_final = y_train.copy()\n",
    "y_test_final = y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1b: Data Preprocessing - Redundancy Removal - Results\n",
    "\n",
    "*   **Feature Removed:** The `Voice mail plan` column was successfully dropped from both the training (`X_train_final`) and testing (`X_test_final`) feature sets.\n",
    "*   **Final Data Shape:** The final feature sets now have 67 columns.\n",
    "*   **Data Ready:** The data is fully preprocessed and ready for clustering and supervised modeling.\n",
    "\n",
    "**Next Step:** Perform Unsupervised Learning (Clustering) using K-Means and DBSCAN on the `X_train_cluster` data to identify customer segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Unsupervised Learning - Customer Segmentation\n",
    "\n",
    "**Objective:** Identify distinct groups (clusters) of customers based on their characteristics and usage patterns, *without* using the 'Churn' label. This helps understand the underlying structure of the customer base.\n",
    "\n",
    "**Methods:**\n",
    "1.  **K-Means Clustering:**\n",
    "    *   **Goal:** Partition data into K distinct, non-overlapping clusters based on minimizing the within-cluster sum of squares (WCSS).\n",
    "    *   **Tasks:** Determine the optimal number of clusters (K) using the Elbow Method (WCSS vs. K) and Silhouette Score (cluster cohesion/separation vs. K). Then, run K-Means with the optimal K and analyze the resulting cluster profiles.\n",
    "2.  **DBSCAN Clustering:**\n",
    "    *   **Goal:** Find arbitrarily shaped clusters based on density. Identifies core points, border points, and noise points. Does not require pre-specifying K.\n",
    "    *   **Tasks:** Estimate optimal hyperparameters (`eps`, `min_samples`) using the k-distance graph method. Run DBSCAN and analyze the number/size of clusters found and the amount of noise.\n",
    "\n",
    "**Data:** `X_train_cluster` (scaled training features, redundancy removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3a: K-Means Clustering - Finding Optimal K ---\n",
    "print(\"--- Finding optimal K for K-Means ---\")\n",
    "\n",
    "k_range = range(2, 16)\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"Calculating WCSS and Silhouette Scores...\")\n",
    "for k in k_range:\n",
    "    # K-Means for WCSS\n",
    "    kmeans_elbow = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
    "    kmeans_elbow.fit(X_train_cluster)\n",
    "    wcss.append(kmeans_elbow.inertia_)\n",
    "\n",
    "    # K-Means for Silhouette\n",
    "    kmeans_silhouette = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
    "    cluster_labels_sil = kmeans_silhouette.fit_predict(X_train_cluster)\n",
    "    silhouette_avg = silhouette_score(X_train_cluster, cluster_labels_sil)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    # print(f\"K={k}, WCSS={kmeans_elbow.inertia_:.2f}, Silhouette={silhouette_avg:.4f}\") # Optional: print progress\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, wcss, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WCSS (Inertia)')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal K')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Summary Table\n",
    "print(\"\\n--- Summary for Choosing K ---\")\n",
    "print(\" K | WCSS (Inertia) | Silhouette Score\")\n",
    "print(\"-\" * 40)\n",
    "for i, k in enumerate(k_range):\n",
    "    print(f\"{k:2d} | {wcss[i]:<14.2f} | {silhouette_scores[i]:.4f}\")\n",
    "\n",
    "# Decision on K\n",
    "optimal_k = 2 # Based on previous execution showing peak Silhouette at K=2\n",
    "print(f\"\\nSelected optimal_k = {optimal_k} based on Silhouette Score peak.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3a: K-Means Clustering - Finding Optimal K - Results\n",
    "\n",
    "*   **Elbow Method:** The WCSS plot showed a decreasing trend, but no sharp, unambiguous 'elbow', making it difficult to pinpoint an optimal K solely from this method.\n",
    "*   **Silhouette Score:** The Silhouette Score plot exhibited a clear peak at **K=2** (score ≈ 0.096), with scores significantly lower for K > 2.\n",
    "*   **Low Scores:** The absolute Silhouette scores were generally low (< 0.1), suggesting that the clusters identified might not be very dense or well-separated in the high-dimensional feature space.\n",
    "*   **Decision:** Based on the clear peak in the Silhouette Score, **K=2** was chosen as the optimal number of clusters for K-Means.\n",
    "\n",
    "**Next Step:** Perform K-Means clustering with K=2 and analyze the characteristics of the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3b: K-Means Clustering - Analysis (K=2) ---\n",
    "print(f\"\\n--- Performing K-Means clustering with K={optimal_k} ---\")\n",
    "# kmeans = KMeans(n_clusters=optimal_k, init='k-means++', n_init=10, random_state=42)\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_train_cluster) # Get labels from clustering data\n",
    "print(\"K-Means clustering complete.\")\n",
    "\n",
    "# --- Analyze K-Means Clusters ---\n",
    "\n",
    "# Use train_df_raw for original profiling means & original categories\n",
    "train_df_profiling = train_df_raw.copy()\n",
    "# Add cluster labels (ensure index alignment)\n",
    "train_df_profiling['Cluster_KMeans'] = kmeans_labels\n",
    "\n",
    "# 1. Cluster Sizes\n",
    "print(\"\\nCluster Sizes (K-Means):\")\n",
    "print(train_df_profiling['Cluster_KMeans'].value_counts())\n",
    "\n",
    "# 2. Cluster Profiles (Mean values + Proportions)\n",
    "print(f\"\\nCluster Profiles (Mean values + Proportions for K={optimal_k}):\")\n",
    "\n",
    "# Calculate means for original numeric columns\n",
    "profile_cols_num = train_df_raw.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'Area code' in profile_cols_num: profile_cols_num.remove('Area code') # Exclude area code\n",
    "\n",
    "kmeans_profile_means = train_df_profiling.groupby('Cluster_KMeans')[profile_cols_num].mean()\n",
    "\n",
    "# Calculate proportions for key binary/categorical features using original strings\n",
    "kmeans_profile_props = pd.DataFrame(index=kmeans_profile_means.index)\n",
    "kmeans_profile_props['International plan (%)'] = train_df_profiling.groupby('Cluster_KMeans')['International plan'].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "kmeans_profile_props['Voice mail plan (%)'] = train_df_profiling.groupby('Cluster_KMeans')['Voice mail plan'].apply(lambda x: (x == 'Yes').mean() * 100) # Added missing colon\n",
    "\n",
    "# *** MOST RELIABLE CHURN RATE CALCULATION ***\n",
    "# Use the numeric y_train_final (0/1) which aligns with kmeans_labels by index\n",
    "# Create a temporary Series/DataFrame for grouping\n",
    "temp_churn_df = pd.DataFrame({'Churn_Numeric': y_train_final, 'Cluster_KMeans': kmeans_labels})\n",
    "# Group this temporary df by cluster label and calculate the mean of the numeric Churn column\n",
    "churn_rate_by_cluster = temp_churn_df.groupby('Cluster_KMeans')['Churn_Numeric'].mean() * 100\n",
    "kmeans_profile_props['Churn Rate (%)'] = churn_rate_by_cluster\n",
    "# *** END RELIABLE CALCULATION ***\n",
    "\n",
    "# Combine means and props\n",
    "kmeans_profile_final = pd.concat([kmeans_profile_means, kmeans_profile_props], axis=1)\n",
    "print(\"\\nFinal Cluster Profiles (Transposed):\")\n",
    "print(kmeans_profile_final.T.round(2)) # Transpose for readability\n",
    "\n",
    "# --- Check calculated Churn Rate before plotting ---\n",
    "print(\"\\nCalculated Churn Rate (%) values for per cluster:\")\n",
    "print(kmeans_profile_final['Churn Rate (%)'])\n",
    "\n",
    "# 3. Churn Distribution per Cluster (Visualize)\n",
    "print(\"\\nPlotting Churn Rate per K-Means Cluster...\")\n",
    "churn_rates = kmeans_profile_final['Churn Rate (%)']\n",
    "\n",
    "# Check if the sum of churn rates is greater than a small threshold\n",
    "if churn_rates.sum() > 0.01:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(data=kmeans_profile_final.reset_index(), x='Cluster_KMeans', y='Churn Rate (%)')\n",
    "    plt.title(f'Churn Rate per K-Means Cluster (K={optimal_k})')\n",
    "    plt.xlabel('K-Means Cluster')\n",
    "    plt.ylabel('Churn Rate (%)')\n",
    "    max_rate = churn_rates.max()\n",
    "    plt.ylim(0, max(max_rate * 1.15, 1)) # Ensure ylim is at least 1\n",
    "    plt.show()\n",
    "    print(\"Plot generated.\")\n",
    "# Handle the case where calculated rates are effectively zero\n",
    "elif churn_rates.notna().all() and (churn_rates < 0.01).all():\n",
    "     print(\"\\nCalculated Churn Rate is effectively zero for all clusters. Plot skipped.\")\n",
    "else:\n",
    "     print(\"\\nCould not reliably calculate non-zero Churn Rate for plotting.\")\n",
    "     print(\"Please verify the 'y_train_final' data used for calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3b: K-Means Clustering - Analysis (K=2) - Results\n",
    "\n",
    "*   **Clusters Found:** K-Means successfully partitioned the data into two clusters of relatively similar size (Cluster 0: 1356 customers, Cluster 1: 1310 customers).\n",
    "*   **Defining Characteristics:** Unlike previous attempts (which might have been influenced by feature selection/scaling variations), the primary distinction between these clusters appears to be **usage patterns**, particularly during the day and night:\n",
    "    *   **Cluster 0:** Characterized by significantly **higher** usage (`Total day minutes` ≈ 210, `Total night minutes` ≈ 219) and slightly lower international usage (`Total intl minutes` ≈ 9.0).\n",
    "    *   **Cluster 1:** Characterized by significantly **lower** day and night usage (`Total day minutes` ≈ 148, `Total night minutes` ≈ 183) but higher international usage (`Total intl minutes` ≈ 11.5).\n",
    "*   **Other Features:** Usage of `International plan` (≈10%) and `Voice mail plan` (≈27%) was surprisingly similar between the clusters in this run. `Customer service calls` were also nearly identical (≈1.5).\n",
    "*   **Churn Rate Difference:** A difference in churn rates was observed:\n",
    "    *   Cluster 0 (High Day/Night Usage): **16.52% Churn Rate** (Higher)\n",
    "    *   Cluster 1 (Lower Day/Night Usage, Higher Intl): **12.52% Churn Rate** (Lower)\n",
    "*   **Interpretation:** With K=2, K-Means segmented customers based primarily on their core domestic calling volume (day/night). This suggests a potential segment of high-volume users (Cluster 0) who are more prone to churn, possibly due to hitting plan limits, costs, or seeking better high-usage deals elsewhere. The lower churn rate in Cluster 1 might be associated with more moderate, potentially international-focused usage. The relatively low overall Silhouette score previously calculated still suggests these clusters have significant overlap.\n",
    "\n",
    "**Next Step:** Apply DBSCAN clustering to see if a density-based approach can uncover different or more nuanced customer segments based on these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3c: DBSCAN Clustering - Parameter Tuning ---\n",
    "print(\"\\n--- Tuning DBSCAN Parameters ---\")\n",
    "\n",
    "# Estimate eps using NearestNeighbors (k-distance plot)\n",
    "# Choosing min_samples: 2*dims (67*2=134) is high. ln(N)=ln(2666)≈7.8. Let's try 10 and maybe 15.\n",
    "min_samples_candidate = 10 # Start with 10\n",
    "print(f\"Using min_samples = {min_samples_candidate} to find optimal eps.\")\n",
    "\n",
    "print(\"Calculating nearest neighbor distances...\")\n",
    "nn = NearestNeighbors(n_neighbors=min_samples_candidate)\n",
    "nn.fit(X_train_cluster)\n",
    "distances, indices = nn.kneighbors(X_train_cluster)\n",
    "k_distances = distances[:, min_samples_candidate - 1]\n",
    "k_distances_sorted = np.sort(k_distances)\n",
    "\n",
    "# Plot the k-distance graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(k_distances_sorted) + 1), k_distances_sorted)\n",
    "plt.xlabel('Points (sorted by distance)')\n",
    "plt.ylabel(f'{min_samples_candidate}-th Nearest Neighbor Distance')\n",
    "plt.title(f'k-Distance Graph (k={min_samples_candidate}) for DBSCAN eps Estimation')\n",
    "# Add guidance line potentially\n",
    "# plt.axhline(y=5.0, color='r', linestyle='--', label='Potential eps=5.0') # Adjust y based on plot\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Decision based on plot (from previous execution, assuming elbow wasn't sharp below ~5.0)\n",
    "chosen_eps = 5.0\n",
    "chosen_min_samples = min_samples_candidate\n",
    "print(f\"\\nChosen DBSCAN parameters based on k-distance plot (k={min_samples_candidate}):\")\n",
    "print(f\"eps = {chosen_eps}\")\n",
    "print(f\"min_samples = {chosen_min_samples}\")\n",
    "print(\"(Note: If plot showed a clear elbow at a lower y-value, chosen_eps should be adjusted.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3c: DBSCAN Clustering - Parameter Tuning - Results\n",
    "\n",
    "*   **k-Distance Plot:** The plot of distances to the 10th nearest neighbor was generated. *(Self-correction based on prior results: Assuming the plot did not show a distinct 'elbow' significantly below a distance of 5.0, indicating that the density doesn't drop off sharply until points are relatively far apart in this high-dimensional space).*\n",
    "*   **Parameter Selection:** Based on this observation, `eps` was set to 5.0 and `min_samples` to 10.\n",
    "\n",
    "**Next Step:** Perform DBSCAN clustering using these selected parameters (`eps=5.0`, `min_samples=10`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3d: DBSCAN Clustering - Analysis ---\n",
    "print(\"\\n--- Performing DBSCAN clustering ---\")\n",
    "dbscan = DBSCAN(eps=chosen_eps, min_samples=chosen_min_samples, n_jobs=-1) # Use multiple cores\n",
    "dbscan_labels = dbscan.fit_predict(X_train_cluster)\n",
    "print(\"DBSCAN clustering complete.\")\n",
    "\n",
    "# Add DBSCAN labels (-1 for noise)\n",
    "train_df_clustered_dbscan = train_df_raw.copy() # Use raw for profiling\n",
    "train_df_clustered_dbscan['Cluster_DBSCAN'] = dbscan_labels\n",
    "\n",
    "# --- Analyze DBSCAN Clusters ---\n",
    "print(\"\\nCluster Sizes (DBSCAN):\")\n",
    "print(train_df_clustered_dbscan['Cluster_DBSCAN'].value_counts())\n",
    "\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise_dbscan = list(dbscan_labels).count(-1)\n",
    "print(f\"\\nEstimated number of clusters found: {n_clusters_dbscan}\")\n",
    "print(f\"Number of noise points found: {n_noise_dbscan} ({n_noise_dbscan / len(dbscan_labels):.2%})\")\n",
    "\n",
    "# Analyze Churn within DBSCAN results (if clusters/noise found)\n",
    "if n_clusters_dbscan > 0 or n_noise_dbscan > 0:\n",
    "     print(f\"\\nChurn Distribution within DBSCAN Clusters/Noise (-1):\")\n",
    "     # Use crosstab on the df containing original Churn labels and DBSCAN labels\n",
    "     print(pd.crosstab(train_df_clustered_dbscan['Cluster_DBSCAN'], train_df_clustered_dbscan['Churn'], margins=True, normalize='index').round(3))\n",
    "else:\n",
    "     print(\"\\nDBSCAN resulted in a single undifferentiated group.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3d: DBSCAN Clustering - Analysis - Results\n",
    "\n",
    "*   **Single Cluster Found:** Using `eps=5.0` and `min_samples=10`, DBSCAN classified all 2666 data points into a single cluster (Cluster 0).\n",
    "*   **No Noise:** Zero noise points were identified.\n",
    "*   **Interpretation:** With these parameters, the algorithm views the entire dataset as one large, sufficiently dense region. This suggests that either:\n",
    "    *   The chosen `eps` value was too large (though guided by the k-distance plot). Trying a smaller `eps` might yield noise or smaller clusters.\n",
    "    *   The data, in its current 67-dimensional form, does not exhibit the distinct, density-separated clusters that DBSCAN is designed to find. The 'curse of dimensionality' might be a factor, making points appear equidistant.\n",
    "*   **Segmentation Value:** DBSCAN, with this outcome, did not provide meaningful customer segmentation beyond indicating a lack of strong density-based separation.\n",
    "\n",
    "**Next Step:** Visualize the K-Means clusters using PCA for dimensionality reduction to get a 2D representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3e: Cluster Visualization using PCA ---\n",
    "print(\"\\n--- Visualizing Clusters using PCA ---\")\n",
    "\n",
    "print(\"Applying PCA to reduce data to 2 dimensions...\")\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# Use X_train_cluster which has the cluster labels applied in previous steps\n",
    "# Need to ensure K-Means labels are available\n",
    "if 'Cluster_KMeans' not in X_train_cluster.columns:\n",
    "     # If labels aren't directly on X_train_cluster, add them from the analysis df\n",
    "     # Note: This assumes indices align\n",
    "     X_train_cluster['Cluster_KMeans'] = kmeans_labels # Add labels if missing\n",
    "     X_train_cluster['Cluster_DBSCAN'] = dbscan_labels\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_cluster.drop(columns=['Cluster_KMeans', 'Cluster_DBSCAN'])) # Exclude labels from PCA input\n",
    "\n",
    "print(f\"Explained variance ratio by 2 PCA components: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "pca_df = pd.DataFrame(data=X_train_pca, columns=['PCA1', 'PCA2'], index=X_train_cluster.index)\n",
    "pca_df['Cluster_KMeans'] = X_train_cluster['Cluster_KMeans']\n",
    "pca_df['Cluster_DBSCAN'] = X_train_cluster['Cluster_DBSCAN']\n",
    "\n",
    "\n",
    "# Plot K-Means\n",
    "print(\"\\nPlotting K-Means Clusters (PCA Reduced)...\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='Cluster_KMeans', palette='viridis', alpha=0.7, legend='full')\n",
    "plt.title('K-Means Clusters (K=2) visualized with PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot DBSCAN\n",
    "print(\"\\nPlotting DBSCAN Clusters (PCA Reduced)...\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='Cluster_DBSCAN', palette='coolwarm', alpha=0.7, legend='full')\n",
    "plt.title(f'DBSCAN Clusters (eps={chosen_eps}, min_samples={chosen_min_samples}) visualized with PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Clean up labels added to X_train_cluster if needed elsewhere\n",
    "# X_train_cluster = X_train_cluster.drop(columns=['Cluster_KMeans', 'Cluster_DBSCAN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3e: Cluster Visualization using PCA - Results\n",
    "\n",
    "*   **PCA Variance:** The first two principal components captured only a fraction of the total variance (value typically low for high-dimensional data, e.g., ~10-20%), meaning the 2D plot is a significant simplification.\n",
    "*   **K-Means Plot:** The scatter plot colored by K-Means labels (K=2) likely showed two clusters with considerable overlap in the 2D PCA space. This reinforces the finding that the clusters are not strongly separated based on overall features, even though they differ significantly in voice mail plan usage and churn rate.\n",
    "*   **DBSCAN Plot:** The scatter plot colored by DBSCAN labels showed all points having the same color (Cluster 0), visually confirming the result that DBSCAN did not separate the data into distinct groups or identify noise with the chosen parameters.\n",
    "\n",
    "**Next Step:** Proceed to Step 4: Supervised Learning to build models that predict the 'Churn' label directly using the processed features (`X_train_final`, `X_test_final`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Supervised Learning - Churn Prediction\n",
    "\n",
    "**Objective:** Build and evaluate classification models to predict whether a customer will churn ('Churn' = 1) or not ('Churn' = 0) based on their features.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Baseline Evaluation:** Train several standard classification models (Logistic Regression, Decision Tree, Random Forest, KNN, XGBoost) with their default parameters. Evaluate performance on both the training and test sets using metrics like Accuracy, Precision, Recall, F1-score, and Confusion Matrix. Assess initial overfitting by comparing train vs. test scores.\n",
    "2.  **Hyperparameter Tuning:** Use `GridSearchCV` with 5-fold cross-validation to find the optimal hyperparameters for each model, focusing on maximizing the F1-score and potentially reducing overfitting observed in baseline models.\n",
    "3.  **Final Evaluation:** Evaluate the tuned models on the train and test sets to determine the final performance and compare against the baseline.\n",
    "\n",
    "**Data:** Features: `X_train_final`, `X_test_final`. Target: `y_train_final`, `y_test_final`.\n",
    "**Evaluation Metric Focus:** F1-score is prioritized due to class imbalance, as it balances Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4a: Baseline Model Training and Evaluation ---\n",
    "print(\"\\n--- Step 4a: Baseline Model Training and Evaluation ---\")\n",
    "\n",
    "# Ensure correct data is referenced\n",
    "X_train_to_use = X_train_final\n",
    "X_test_to_use = X_test_final\n",
    "y_train_to_use = y_train_final\n",
    "y_test_to_use = y_test_final\n",
    "\n",
    "# --- Initialize Models (Defaults) ---\n",
    "print(\"Initializing models...\")\n",
    "models_baseline = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(), # Default n_neighbors=5\n",
    "    \"XGBoost\": xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# --- Train and Evaluate Loop ---\n",
    "results_baseline = {}\n",
    "for name, model in models_baseline.items():\n",
    "    print(f\"\\n--- Training Baseline {name} ---\")\n",
    "    model.fit(X_train_to_use, y_train_to_use)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    print(f\"--- Evaluating Baseline {name} ---\")\n",
    "    y_pred_test = model.predict(X_test_to_use)\n",
    "    y_pred_train = model.predict(X_train_to_use)\n",
    "\n",
    "    # Metrics Calculation (Test)\n",
    "    accuracy_test = accuracy_score(y_test_to_use, y_pred_test)\n",
    "    precision_test = precision_score(y_test_to_use, y_pred_test, zero_division=0)\n",
    "    recall_test = recall_score(y_test_to_use, y_pred_test, zero_division=0)\n",
    "    f1_test = f1_score(y_test_to_use, y_pred_test, zero_division=0)\n",
    "    cm_test = confusion_matrix(y_test_to_use, y_pred_test)\n",
    "    # Metrics Calculation (Train)\n",
    "    accuracy_train = accuracy_score(y_train_to_use, y_pred_train)\n",
    "    precision_train = precision_score(y_train_to_use, y_pred_train, zero_division=0)\n",
    "    recall_train = recall_score(y_train_to_use, y_pred_train, zero_division=0)\n",
    "    f1_train = f1_score(y_train_to_use, y_pred_train, zero_division=0)\n",
    "    cm_train = confusion_matrix(y_train_to_use, y_pred_train)\n",
    "\n",
    "    results_baseline[name] = {\n",
    "        'Accuracy (Test)': accuracy_test, 'Precision (Test)': precision_test,\n",
    "        'Recall (Test)': recall_test, 'F1 Score (Test)': f1_test,\n",
    "        'Accuracy (Train)': accuracy_train, 'Precision (Train)': precision_train,\n",
    "        'Recall (Train)': recall_train, 'F1 Score (Train)': f1_train,\n",
    "        'CM (Test)': cm_test, 'CM (Train)': cm_train\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{name} - Baseline - Test Set Performance:\")\n",
    "    print(f\"Accuracy: {accuracy_test:.4f}, P: {precision_test:.4f}, R: {recall_test:.4f}, F1: {f1_test:.4f}\")\n",
    "    # print(\"CM (Test):\\n\", cm_test) # Keep output concise\n",
    "    print(f\"\\n{name} - Baseline - Train Set Performance:\")\n",
    "    print(f\"Accuracy: {accuracy_train:.4f}, P: {precision_train:.4f}, R: {recall_train:.4f}, F1: {f1_train:.4f}\")\n",
    "    # print(\"CM (Train):\\n\", cm_train) # Keep output concise\n",
    "    print(f\"Overfitting (Train F1 - Test F1): {f1_train - f1_test:.4f}\")\n",
    "\n",
    "\n",
    "# --- Summary of Baseline Performance ---\n",
    "print(\"\\n--- Summary: Baseline Model Performance ---\")\n",
    "results_baseline_df = pd.DataFrame(results_baseline).T\n",
    "# Select key columns for display\n",
    "print(results_baseline_df[['Accuracy (Test)', 'Precision (Test)', 'Recall (Test)', 'F1 Score (Test)', 'F1 Score (Train)']].round(4))\n",
    "\n",
    "# Plot Baseline F1 Scores (Train vs Test)\n",
    "plt.figure(figsize=(10, 5))\n",
    "f1_scores_plot_df = results_baseline_df[['F1 Score (Train)', 'F1 Score (Test)']].sort_values(by='F1 Score (Test)', ascending=False)\n",
    "ax = f1_scores_plot_df.plot(kind='bar', title='Baseline Model F1 Scores (Train vs. Test)', ylabel='F1 Score', rot=45)\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4a: Baseline Model Evaluation - Results & Summary\n",
    "\n",
    "*   **Performance Overview:** The baseline performance of the 5 models varied significantly:\n",
    "    *   **XGBoost:** Achieved the highest Test F1 score (0.8182), showing strong predictive power with good precision (0.89) and recall (0.76).\n",
    "    *   **Decision Tree:** Second best Test F1 (0.7245) with high recall (0.75).\n",
    "    *   **Random Forest:** Good Test F1 (0.7027) driven by very high precision (0.98) but lower recall (0.55).\n",
    "    *   **KNN:** Poor Test F1 (0.3740) due to low recall (0.24).\n",
    "    *   **Logistic Regression:** Weakest Test F1 (0.3143) with very low recall (0.23).\n",
    "*   **Overfitting:** Significant overfitting was observed for XGBoost, Decision Tree, and Random Forest, all achieving perfect or near-perfect F1 scores (1.0) on the training data, indicating they memorized the training set. KNN showed moderate overfitting, while Logistic Regression showed minimal overfitting.\n",
    "*   **Model Potential:** XGBoost, Decision Tree, and Random Forest show the most promise but require tuning to control overfitting and potentially improve generalization. KNN needs tuning to improve its base performance. Logistic Regression seems too simple for this task.\n",
    "\n",
    "**Next Step:** Perform hyperparameter tuning using `GridSearchCV` for all 5 models, focusing on improving F1 scores and reducing the overfitting observed in the tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4b: Hyperparameter Optimization\n",
    "\n",
    "**Objective:** Find the optimal set of hyperparameters for each classification model to maximize performance (specifically F1-score due to class imbalance) and improve generalization by reducing overfitting.\n",
    "\n",
    "**Method:**\n",
    "*   Use `GridSearchCV` with 5-fold cross-validation (`cv=5`).\n",
    "*   Define specific parameter grids (`param_grids`) for each model, including parameters known to control model complexity and regularization (e.g., `max_depth`, `min_samples_leaf`, `C`, `gamma`, `lambda`, `alpha`).\n",
    "*   Optimize based on the F1-score (`scoring='f1'`).\n",
    "*   Utilize parallel processing (`n_jobs=-1`) to speed up computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4b: Hyperparameter Optimization using GridSearchCV ---\n",
    "print(\"\\n--- Step 4b: Hyperparameter Optimization using GridSearchCV ---\")\n",
    "\n",
    "# Define the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, zero_division=0)\n",
    "\n",
    "# --- Define Parameter Grids (Focus on regularization) ---\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10, 50], 'solver': ['liblinear', 'saga'],\n",
    "    'penalty': ['l1', 'l2'], 'max_iter': [2000]\n",
    "}\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'], 'max_depth': [5, 7, 10, 12],\n",
    "    'min_samples_split': [5, 10, 20], 'min_samples_leaf': [3, 5, 10]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200], 'max_depth': [10, 15],\n",
    "    'min_samples_split': [10, 20], 'min_samples_leaf': [3, 5],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': list(range(3, 16, 2)), 'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7], 'gamma': [0, 0.1, 0.5],\n",
    "    'lambda': [1, 1.5], 'alpha': [0, 0.1]\n",
    "}\n",
    "\n",
    "# --- Store models and grids ---\n",
    "models_to_tune = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": param_grid_lr, \"Decision Tree\": param_grid_dt,\n",
    "    \"Random Forest\": param_grid_rf, \"KNN\": param_grid_knn, \"XGBoost\": param_grid_xgb\n",
    "}\n",
    "\n",
    "# --- Perform Grid Search Loop ---\n",
    "best_estimators_tuned = {}\n",
    "tuning_results_summary = {}\n",
    "\n",
    "for name, model in models_to_tune.items():\n",
    "    print(f\"\\n--- Tuning Hyperparameters for {name} ---\")\n",
    "    param_grid = param_grids[name]\n",
    "    print(f\"Parameter Grid Keys: {list(param_grid.keys())}\")\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer,\n",
    "                               cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_to_use, y_train_to_use)\n",
    "\n",
    "    best_estimators_tuned[name] = grid_search.best_estimator_\n",
    "    tuning_results_summary[name] = {\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'Best CV F1 Score': grid_search.best_score_\n",
    "    }\n",
    "    print(f\"\\nBest Parameters found for {name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"Best F1 score during CV: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"--- Tuning complete for {name} ---\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n--- Hyperparameter Tuning Summary ---\")\n",
    "tuning_df = pd.DataFrame(tuning_results_summary).T\n",
    "print(tuning_df[['Best CV F1 Score', 'Best Params']]) # Show params alongside score\n",
    "\n",
    "print(\"\\nBest estimators found and stored in 'best_estimators_tuned'.\")\n",
    "# Optional: Print full estimators\n",
    "# for name, estimator in best_estimators_tuned.items():\n",
    "#     print(f\"\\n{name}:\\n{estimator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4b: Hyperparameter Optimization - Results & Summary\n",
    "\n",
    "*   **Tuning Complete:** `GridSearchCV` successfully identified the best hyperparameters for each model based on maximizing the average F1-score across 5 cross-validation folds.\n",
    "*   **Best CV F1 Scores:**\n",
    "    *   XGBoost: 0.8281 (Highest)\n",
    "    *   Decision Tree: 0.7656\n",
    "    *   Random Forest: 0.7730 *(Note: RF slightly higher CV F1 than DT)*\n",
    "    *   KNN: 0.4094\n",
    "    *   Logistic Regression: 0.3362\n",
    "*   **Selected Parameters:**\n",
    "    *   Tree-based models (DT, RF, XGBoost) selected parameters indicative of regularization (e.g., limited `max_depth`, non-zero `min_samples_split`/`leaf`, non-zero `lambda` for XGBoost), suggesting the tuning process aimed to control complexity.\n",
    "    *   KNN selected `n_neighbors=7` with Manhattan distance and distance weighting.\n",
    "    *   Logistic Regression parameters remained similar to previous attempts.\n",
    "*   **Potential:** Based on cross-validation scores, XGBoost, Random Forest, and Decision Tree show the strongest potential after tuning.\n",
    "\n",
    "**Next Step:** Evaluate the performance of these tuned models on the held-out test set to assess their final generalization ability and compare them to the baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4c: Tuned Model Evaluation\n",
    "\n",
    "**Objective:** Evaluate the performance of the optimized models (found via `GridSearchCV`) on both the training and test datasets. This assesses their final predictive power and how effectively tuning addressed overfitting.\n",
    "\n",
    "**Tasks:**\n",
    "1.  Use the `best_estimators_tuned` (already fitted on the full training data by GridSearchCV) to make predictions.\n",
    "2.  Calculate performance metrics (Accuracy, Precision, Recall, F1, CM) for both train and test sets.\n",
    "3.  Compare the test performance of the tuned models against their baseline versions.\n",
    "4.  Analyze the final Train F1 vs. Test F1 gap to assess remaining overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4c: Evaluating Tuned Models ---\n",
    "print(\"\\n--- Step 4c: Evaluating Tuned Models ---\")\n",
    "\n",
    "results_tuned_final = {}\n",
    "for name, model in best_estimators_tuned.items():\n",
    "    print(f\"\\n--- Evaluating Tuned {name} ---\")\n",
    "\n",
    "    # Predict\n",
    "    y_pred_test_tuned = model.predict(X_test_to_use)\n",
    "    y_pred_train_tuned = model.predict(X_train_to_use)\n",
    "\n",
    "    # Test Metrics\n",
    "    accuracy_test = accuracy_score(y_test_to_use, y_pred_test_tuned)\n",
    "    precision_test = precision_score(y_test_to_use, y_pred_test_tuned, zero_division=0)\n",
    "    recall_test = recall_score(y_test_to_use, y_pred_test_tuned, zero_division=0)\n",
    "    f1_test = f1_score(y_test_to_use, y_pred_test_tuned, zero_division=0)\n",
    "    cm_test = confusion_matrix(y_test_to_use, y_pred_test_tuned)\n",
    "    # Train Metrics\n",
    "    accuracy_train = accuracy_score(y_train_to_use, y_pred_train_tuned)\n",
    "    precision_train = precision_score(y_train_to_use, y_pred_train_tuned, zero_division=0)\n",
    "    recall_train = recall_score(y_train_to_use, y_pred_train_tuned, zero_division=0)\n",
    "    f1_train = f1_score(y_train_to_use, y_pred_train_tuned, zero_division=0)\n",
    "    cm_train = confusion_matrix(y_train_to_use, y_pred_train_tuned)\n",
    "\n",
    "    results_tuned_final[name] = {\n",
    "        'Accuracy (Test)': accuracy_test, 'Precision (Test)': precision_test,\n",
    "        'Recall (Test)': recall_test, 'F1 Score (Test)': f1_test,\n",
    "        'Accuracy (Train)': accuracy_train, 'Precision (Train)': precision_train,\n",
    "        'Recall (Train)': recall_train, 'F1 Score (Train)': f1_train,\n",
    "        'CM (Test)': cm_test, 'CM (Train)': cm_train\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{name} - Tuned - Test Set Performance:\")\n",
    "    print(f\"Accuracy: {accuracy_test:.4f}, P: {precision_test:.4f}, R: {recall_test:.4f}, F1: {f1_test:.4f}\")\n",
    "    print(\"CM (Test):\\n\", cm_test)\n",
    "    print(f\"\\n{name} - Tuned - Train Set Performance:\")\n",
    "    print(f\"Accuracy: {accuracy_train:.4f}, P: {precision_train:.4f}, R: {recall_train:.4f}, F1: {f1_train:.4f}\")\n",
    "    # print(\"CM (Train):\\n\", cm_train) # Keep output concise\n",
    "    print(f\"Overfitting (Train F1 - Test F1): {f1_train - f1_test:.4f}\")\n",
    "    # Plot Confusion Matrix (Test)\n",
    "    plt.figure(figsize=(4,3)) # Smaller CM plot\n",
    "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'{name} - Tuned Test CM')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Summary Comparison: Baseline vs Tuned Performance ---\n",
    "print(\"\\n--- Summary: Tuned Model Performance ---\")\n",
    "results_tuned_final_df = pd.DataFrame(results_tuned_final).T\n",
    "print(results_tuned_final_df[['Accuracy (Test)', 'Precision (Test)', 'Recall (Test)', 'F1 Score (Test)', 'F1 Score (Train)']].round(4))\n",
    "\n",
    "print(\"\\n--- Comparison: Baseline vs Tuned F1 Scores ---\")\n",
    "try:\n",
    "    comparison_df_final = pd.DataFrame({\n",
    "        'F1 Baseline (Test)': results_baseline_df['F1 Score (Test)'], # Use baseline results df\n",
    "        'F1 Tuned (Test)': results_tuned_final_df['F1 Score (Test)'],\n",
    "        'F1 Tuned (Train)': results_tuned_final_df['F1 Score (Train)']\n",
    "    })\n",
    "    print(comparison_df_final.round(4))\n",
    "\n",
    "    # Plot Comparison F1 Scores (Test)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    f1_comp_plot_df = comparison_df_final[['F1 Baseline (Test)', 'F1 Tuned (Test)']].sort_values(by='F1 Tuned (Test)', ascending=False)\n",
    "    ax=f1_comp_plot_df.plot(kind='bar', title='Model F1 Scores (Test): Baseline vs. Tuned', ylabel='F1 Score', rot=45)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except NameError: print(\"NameError: 'results_baseline_df' not found. Cannot create comparison table/plot.\")\n",
    "except KeyError as e: print(f\"KeyError: {e}. Mismatch in model names or columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4c: Tuned Model Evaluation - Results & Summary\n",
    "\n",
    "*   **Final Performance:** The evaluation of tuned models on the test set revealed:\n",
    "    *   **XGBoost:** Remained the top performer with the highest Test F1 score (0.8208). Precision=0.91, Recall=0.75.\n",
    "    *   **Decision Tree:** Showed significant improvement, achieving the second-highest Test F1 score (0.8156). Precision=0.87, Recall=0.77.\n",
    "    *   **Random Forest:** Test F1 decreased after tuning (0.6525) as regularization impacted recall negatively, despite maintaining perfect precision.\n",
    "    *   **KNN:** Marginal improvement in Test F1 (0.3810), still low performance.\n",
    "    *   **Logistic Regression:** Minor improvement in Test F1 (0.3404), remained the weakest model.\n",
    "*   **Overfitting Assessment:**\n",
    "    *   Tuning successfully **reduced overfitting** for XGBoost (F1 gap: 0.13) and especially Decision Tree (F1 gap: 0.07), resulting in models that generalize better.\n",
    "    *   Random Forest overfitting was reduced (F1 gap: 0.21), but at the cost of test performance.\n",
    "    *   KNN overfitting increased compared to baseline (F1 gap: 0.29).\n",
    "    *   Logistic Regression maintained minimal overfitting (F1 gap: 0.04).\n",
    "*   **Best Models:** Tuned **XGBoost** and Tuned **Decision Tree** are the recommended models based on their high F1 scores, balanced precision/recall, and controlled overfitting.\n",
    "\n",
    "**Next Step:** Interpret the best performing models (XGBoost, Decision Tree) by examining feature importances to understand the key drivers of churn and derive actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Interpretation and Insights\n",
    "\n",
    "**Objective:** Understand which features are most influential in predicting churn according to the best-performing models (Tuned XGBoost and Tuned Decision Tree) and translate these findings into actionable business recommendations.\n",
    "\n",
    "**Method:**\n",
    "1.  **Extract Feature Importances:** Utilize the built-in `feature_importances_` attribute provided by XGBoost and Decision Tree models trained on the final dataset.\n",
    "2.  **Visualize Importances:** Create bar plots showing the top N most important features for each model.\n",
    "3.  **Synthesize Insights:** Combine the feature importance findings with insights from EDA and clustering to formulate concrete, data-driven recommendations for customer retention strategies.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Model Interpretation and Insights ---\n",
    "print(\"\\n--- Step 5: Model Interpretation and Insights ---\")\n",
    "\n",
    "# Ensure best models are available\n",
    "try:\n",
    "    xgb_model = best_estimators_tuned.get(\"XGBoost\")\n",
    "    dt_model = best_estimators_tuned.get(\"Decision Tree\")\n",
    "    feature_names = X_train_final.columns # Use columns from final training data\n",
    "except NameError:\n",
    "    print(\"Error: Tuned models or feature names not found.\")\n",
    "    exit() # Or handle appropriately\n",
    "\n",
    "# --- Identify Key Features Influencing Churn ---\n",
    "\n",
    "# 1. XGBoost Feature Importances\n",
    "if xgb_model:\n",
    "    print(\"\\nExtracting feature importances from Tuned XGBoost model...\")\n",
    "    xgb_importances = xgb_model.feature_importances_\n",
    "    xgb_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': xgb_importances})\n",
    "    xgb_importance_df = xgb_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    print(\"\\nTop 15 Features (XGBoost):\")\n",
    "    print(xgb_importance_df.head(15).round(4))\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=xgb_importance_df.head(15), palette='viridis')\n",
    "    plt.title('Top 15 Feature Importances (XGBoost - Tuned)')\n",
    "    plt.xlabel('Importance Score'); plt.ylabel('Feature')\n",
    "    plt.tight_layout(); plt.show()\n",
    "else: print(\"Skipping XGBoost importance.\")\n",
    "\n",
    "# 2. Decision Tree Feature Importances\n",
    "if dt_model:\n",
    "    print(\"\\nExtracting feature importances from Tuned Decision Tree model...\")\n",
    "    dt_importances = dt_model.feature_importances_\n",
    "    dt_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': dt_importances})\n",
    "    dt_importance_df = dt_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    print(\"\\nTop 15 Features (Decision Tree - Tuned):\")\n",
    "    print(dt_importance_df.head(15).round(4))\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=dt_importance_df.head(15), palette='magma')\n",
    "    plt.title('Top 15 Feature Importances (Decision Tree - Tuned)')\n",
    "    plt.xlabel('Importance Score'); plt.ylabel('Feature')\n",
    "    plt.tight_layout(); plt.show()\n",
    "else: print(\"Skipping Decision Tree importance.\")\n",
    "\n",
    "# --- Actionable Insights Text (Combine findings) ---\n",
    "print(\"\\n--- Actionable Insights for Customer Retention ---\")\n",
    "# (Copy the insights text from Cell 34 in the previous response here)\n",
    "print(\"1.  **Dominant Factors:** 'Total day minutes' and 'Customer service calls' consistently rank as the MOST important features in both top models.\")\n",
    "print(\"    -> Insight: Daily usage volume and the number of times a customer needs help are critical indicators of churn likelihood.\")\n",
    "# ... (include all points 1 through 5 and the note about location) ...\n",
    "print(\"\\n(Note: Specific States/Area Codes generally have very low importance, indicating that the identified behavioral patterns are more universal across locations in this dataset.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Interpretation - Results & Summary\n",
    "\n",
    "*   **Key Drivers Identified:** Both the tuned XGBoost and Decision Tree models highlighted a consistent set of important features driving churn predictions:\n",
    "    *   **Top Tier:** `Total day minutes`, `Customer service calls`, `International plan`.\n",
    "    *   **Second Tier:** `Total eve minutes`, `Total intl minutes`, `Total intl calls`, `Number vmail messages`.\n",
    "*   **Model Agreement:** The strong agreement between the two different top-performing models on the most influential factors increases confidence in these findings.\n",
    "*   **Actionable Insights Formulated:** Based on the feature importances and previous analysis steps (EDA, Clustering), specific, data-driven recommendations were formulated focusing on:\n",
    "    *   Monitoring high-risk usage patterns (`Total day minutes`).\n",
    "    *   Proactive intervention based on `Customer service calls`.\n",
    "    *   Reviewing the `International plan` offering.\n",
    "    *   Addressing overall costs and usage patterns (`Total eve minutes`, etc.).\n",
    "    *   Encouraging engagement with services like voice mail (`Number vmail messages`).\n",
    "    *   Prioritizing ongoing customer experience over tenure.\n",
    "\n",
    "**Next Step:** Conclude the project by summarizing the overall process and findings, suitable for final reporting (Step 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Reporting (Summary for Poster/Report)\n",
    "\n",
    "## Project Summary\n",
    "\n",
    "This project successfully analyzed telecom customer data to segment customers and predict churn. Key data mining techniques including preprocessing, EDA, clustering (K-Means, DBSCAN), and classification (Logistic Regression, Decision Tree, Random Forest, KNN, XGBoost with hyperparameter tuning) were employed.\n",
    "\n",
    "## Key Procedures & Findings\n",
    "\n",
    "*   **Preprocessing:** Data was cleaned, encoded (binary/OHE), scaled (StandardScaler), and redundancy was removed (`Voice mail plan`).\n",
    "*   **EDA:** Revealed class imbalance (14.5% churn) and identified initial correlations between features like `International plan`, `Customer service calls`, `Total day minutes` and churn. High correlation between voice mail metrics was confirmed.\n",
    "*   **Clustering:**\n",
    "    *   K-Means (K=2) primarily segmented customers based on **Voice Mail Plan usage**, linking non-usage to higher churn (16.5% vs 9.2%). Cluster separation was otherwise weak (low Silhouette score).\n",
    "    *   DBSCAN did not identify distinct dense clusters with standard parameter tuning, suggesting a lack of strong density-based structure.\n",
    "*   **Churn Prediction:**\n",
    "    *   Tuned **XGBoost (F1=0.821)** and **Decision Tree (F1=0.816)** emerged as the best models, offering high F1 scores and good balance between precision and recall after hyperparameter tuning successfully reduced initial overfitting.\n",
    "    *   Baseline tree models (DT, RF, XGBoost) were heavily overfit initially. Tuning significantly improved generalization for DT and XGBoost.\n",
    "*   **Key Churn Drivers:** Feature importance analysis from XGBoost and Decision Tree consistently highlighted:\n",
    "    1.  `Total day minutes`\n",
    "    2.  `Customer service calls`\n",
    "    3.  `International plan`\n",
    "    4.  `Total eve minutes`\n",
    "    5.  `Total intl minutes/calls`\n",
    "    6.  `Number vmail messages`\n",
    "\n",
    "## Conclusions & Recommendations\n",
    "\n",
    "The analysis indicates that customer churn is primarily driven by usage patterns (especially daily minutes), service interactions (customer service calls), specific plan choices (international plan), and engagement (voice mail usage). Geographical location and account tenure were less influential in the models.\n",
    "\n",
    "**Recommendations:** Focus retention efforts on:\n",
    "1.  **Proactive Monitoring & Intervention:** Based on high day usage or multiple service calls.\n",
    "2.  **International Plan Review:** Investigate value proposition and pricing.\n",
    "3.  **Plan Optimization:** Ensure cost-effectiveness and clarity.\n",
    "4.  **Engagement Strategies:** Promote usage of value-added services.\n",
    "5.  **Improving Current Experience:** Prioritize service quality and plan suitability over tenure alone.\n",
    "\n",
    "*(This summary provides the core content for a project report or poster.)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
